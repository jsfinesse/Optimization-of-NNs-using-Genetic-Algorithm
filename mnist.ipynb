{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwAwzXpaXNze"
      },
      "source": [
        "### Import libraries and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nr-Yx4BZXNzi"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.python.keras.utils.np_utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUOKdz0tXNzk"
      },
      "source": [
        "### Preprocess data, normalize images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n5iywLRXNzk",
        "outputId": "1f313b1e-4d3d-4398-b85f-cfb22f171499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "shape of X_train:  (60000, 28, 28)\n",
            "shape of X_test:  (10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "print(\"shape of X_train: \", X_train.shape)\n",
        "print(\"shape of X_test: \", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M9IA5cfXNzm"
      },
      "source": [
        "### Create ANN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCnQAg-1XNzm",
        "outputId": "faa2e510-2efd-4cb6-d51d-9a1e04441d94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               100480    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 101,770\n",
            "Trainable params: 101,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "\n",
        "def init():\n",
        "    model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation= 'relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "modelInfo = init()\n",
        "modelInfo.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = init()\n",
        "r = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VymG52KN-cQ7",
        "outputId": "189dd4f3-abc1-4ba7-fa85-c6af8314af79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5031 - accuracy: 0.8236 - val_loss: 0.4233 - val_accuracy: 0.8480\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3766 - accuracy: 0.8638 - val_loss: 0.3950 - val_accuracy: 0.8566\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3356 - accuracy: 0.8781 - val_loss: 0.4077 - val_accuracy: 0.8579\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3099 - accuracy: 0.8868 - val_loss: 0.3650 - val_accuracy: 0.8665\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2926 - accuracy: 0.8908 - val_loss: 0.3604 - val_accuracy: 0.8722\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2798 - accuracy: 0.8966 - val_loss: 0.3605 - val_accuracy: 0.8699\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2658 - accuracy: 0.9004 - val_loss: 0.3598 - val_accuracy: 0.8733\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2554 - accuracy: 0.9052 - val_loss: 0.3548 - val_accuracy: 0.8733\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2456 - accuracy: 0.9089 - val_loss: 0.3413 - val_accuracy: 0.8784\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2377 - accuracy: 0.9112 - val_loss: 0.3384 - val_accuracy: 0.8818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVkNTqvr-7Xl",
        "outputId": "f57bc649-2a16-4d56-d5c7-9f9b6427d4b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3384 - accuracy: 0.8818\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.338382363319397, 0.8817999958992004]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEsjhXTHXNzn"
      },
      "source": [
        "### Train model for 1 epoch and return models with current loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ_WDzqdXNzo"
      },
      "outputs": [],
      "source": [
        "def train(models):\n",
        "  \n",
        "    losses = []\n",
        "     \n",
        "    for i in range(len(models)):\n",
        "        history = models[i].fit(x=X_train,y=y_train, epochs=1, validation_data=(X_test, y_test))\n",
        "        losses.append(round(history.history['loss'][-1], 4))\n",
        "        \n",
        "    return models, losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rskUFDzXNzo"
      },
      "outputs": [],
      "source": [
        "no_of_generations = 10\n",
        "no_of_individuals = 10\n",
        "mutate_factor = 0.1\n",
        "individuals = []\n",
        "\n",
        "layers = [1, 2]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define mutate function"
      ],
      "metadata": {
        "id": "1DWF1MDp0rP5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBdoWG_zXNzp"
      },
      "outputs": [],
      "source": [
        "def mutate(new_individual):\n",
        "    for layer in layers:\n",
        "        for bias in range(len(new_individual.layers[layer].get_weights()[1])):\n",
        "            n = random.random()\n",
        "            if n < mutate_factor:\n",
        "                new_individual.layers[layer].get_weights(\n",
        "                )[1][bias] *= random.uniform(-0.5, 0.5)\n",
        "\n",
        "    for layer in layers:\n",
        "        for weight in new_individual.layers[layer].get_weights()[0]:\n",
        "            n = random.random()\n",
        "            if n < mutate_factor:\n",
        "                for j in range(len(weight)):\n",
        "                    if random.random() < mutate_factor:\n",
        "                        new_individual.layers[layer].get_weights(\n",
        "                        )[0][j] *= random.uniform(-0.5, 0.5)\n",
        "\n",
        "    return new_individual"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define crossover function"
      ],
      "metadata": {
        "id": "unPdHHDW0zTh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akm-n1ahXNzp"
      },
      "outputs": [],
      "source": [
        "def crossover(individuals_param):\n",
        "    new_individuals = [individuals_param[0], individuals_param[1]]\n",
        "\n",
        "    for j in range(2, no_of_individuals):\n",
        "        if j < (no_of_individuals - 2):\n",
        "            if j == 2:\n",
        "                parentA = random.choice(individuals_param[:3])\n",
        "                parentB = random.choice(individuals_param[:3])\n",
        "            else:\n",
        "                parentA = random.choice(individuals_param[:])\n",
        "                parentB = random.choice(individuals_param[:])\n",
        "\n",
        "            for j in layers:\n",
        "                temp = parentA.layers[j].get_weights()[1]\n",
        "                parentA.layers[j].get_weights()[1] = parentB.layers[j].get_weights()[1]\n",
        "                parentB.layers[j].get_weights()[1] = temp\n",
        "\n",
        "            new_individual = random.choice([parentA, parentB])\n",
        "\n",
        "        else:\n",
        "            new_individual = random.choice(individuals_param[:])\n",
        "\n",
        "        new_individuals.append(mutate(new_individual))\n",
        "        # new_individuals.append(new_individual)\n",
        "\n",
        "    return new_individuals"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define evolve function"
      ],
      "metadata": {
        "id": "LGPHtKAs4pDv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsvGSUG0XNzq"
      },
      "outputs": [],
      "source": [
        "def evolve(individuals_param, losses_param):\n",
        "    sorted_y_idx_list = sorted(range(len(losses_param)), key=lambda x: losses_param[x])\n",
        "    individuals_param = [individuals_param[x] for x in sorted_y_idx_list]\n",
        "\n",
        "    # winners = individuals[:6]\n",
        "\n",
        "    new_individuals = crossover(individuals_param)\n",
        "\n",
        "    return new_individuals\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train individuals and evolve"
      ],
      "metadata": {
        "id": "AZPKQ6Ve4sxs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo94kJY3XNzq",
        "outputId": "a74eec14-2db0-40ca-e8d7-209d0ed14184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.4992 - accuracy: 0.8241 - val_loss: 0.4241 - val_accuracy: 0.8507\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4966 - accuracy: 0.8263 - val_loss: 0.4520 - val_accuracy: 0.8371\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.4989 - accuracy: 0.8237 - val_loss: 0.4166 - val_accuracy: 0.8523\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4953 - accuracy: 0.8261 - val_loss: 0.4242 - val_accuracy: 0.8506\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5038 - accuracy: 0.8228 - val_loss: 0.4209 - val_accuracy: 0.8523\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.4945 - accuracy: 0.8253 - val_loss: 0.4233 - val_accuracy: 0.8481\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.4985 - accuracy: 0.8233 - val_loss: 0.4156 - val_accuracy: 0.8537\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.5036 - accuracy: 0.8220 - val_loss: 0.4235 - val_accuracy: 0.8498\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4972 - accuracy: 0.8264 - val_loss: 0.4293 - val_accuracy: 0.8452\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5014 - accuracy: 0.8246 - val_loss: 0.4335 - val_accuracy: 0.8464\n",
            "[0.4992, 0.4966, 0.4989, 0.4953, 0.5038, 0.4945, 0.4985, 0.5036, 0.4972, 0.5014]\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3751 - accuracy: 0.8645 - val_loss: 0.4171 - val_accuracy: 0.8492\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3747 - accuracy: 0.8642 - val_loss: 0.4001 - val_accuracy: 0.8556\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3371 - accuracy: 0.8760 - val_loss: 0.3737 - val_accuracy: 0.8652\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3138 - accuracy: 0.8846 - val_loss: 0.3513 - val_accuracy: 0.8734\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3707 - accuracy: 0.8659 - val_loss: 0.3883 - val_accuracy: 0.8594\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3736 - accuracy: 0.8659 - val_loss: 0.4192 - val_accuracy: 0.8517\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3352 - accuracy: 0.8772 - val_loss: 0.3670 - val_accuracy: 0.8677\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3346 - accuracy: 0.8783 - val_loss: 0.3619 - val_accuracy: 0.8699\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2962 - accuracy: 0.8907 - val_loss: 0.3492 - val_accuracy: 0.8748\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3337 - accuracy: 0.8777 - val_loss: 0.3764 - val_accuracy: 0.8598\n",
            "[0.3751, 0.3747, 0.3371, 0.3138, 0.3707, 0.3736, 0.3352, 0.3346, 0.2962, 0.3337]\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2824 - accuracy: 0.8954 - val_loss: 0.3503 - val_accuracy: 0.8777\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2696 - accuracy: 0.8984 - val_loss: 0.3412 - val_accuracy: 0.8778\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2600 - accuracy: 0.9022 - val_loss: 0.3382 - val_accuracy: 0.8793\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3111 - accuracy: 0.8863 - val_loss: 0.3526 - val_accuracy: 0.8702\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.2940 - accuracy: 0.8907 - val_loss: 0.3479 - val_accuracy: 0.8765\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3119 - accuracy: 0.8860 - val_loss: 0.3504 - val_accuracy: 0.8723\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2510 - accuracy: 0.9055 - val_loss: 0.3448 - val_accuracy: 0.8797\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2414 - accuracy: 0.9091 - val_loss: 0.3289 - val_accuracy: 0.8859\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2337 - accuracy: 0.9131 - val_loss: 0.3340 - val_accuracy: 0.8832\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2947 - accuracy: 0.8914 - val_loss: 0.3473 - val_accuracy: 0.8736\n",
            "[0.2824, 0.2696, 0.26, 0.3111, 0.294, 0.3119, 0.251, 0.2414, 0.2337, 0.2947]\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2263 - accuracy: 0.9152 - val_loss: 0.3409 - val_accuracy: 0.8842\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2185 - accuracy: 0.9178 - val_loss: 0.3376 - val_accuracy: 0.8840\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2136 - accuracy: 0.9193 - val_loss: 0.3751 - val_accuracy: 0.8702\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2064 - accuracy: 0.9218 - val_loss: 0.3407 - val_accuracy: 0.8891\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.1989 - accuracy: 0.9264 - val_loss: 0.3280 - val_accuracy: 0.8919\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1960 - accuracy: 0.9262 - val_loss: 0.3441 - val_accuracy: 0.8855\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2795 - accuracy: 0.8971 - val_loss: 0.3318 - val_accuracy: 0.8806\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1896 - accuracy: 0.9289 - val_loss: 0.3504 - val_accuracy: 0.8876\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1861 - accuracy: 0.9293 - val_loss: 0.3363 - val_accuracy: 0.8912\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2671 - accuracy: 0.9010 - val_loss: 0.3416 - val_accuracy: 0.8765\n",
            "[0.2263, 0.2185, 0.2136, 0.2064, 0.1989, 0.196, 0.2795, 0.1896, 0.1861, 0.2671]\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1810 - accuracy: 0.9323 - val_loss: 0.3566 - val_accuracy: 0.8869\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1765 - accuracy: 0.9328 - val_loss: 0.3644 - val_accuracy: 0.8843\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1723 - accuracy: 0.9357 - val_loss: 0.3770 - val_accuracy: 0.8834\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.1674 - accuracy: 0.9377 - val_loss: 0.3762 - val_accuracy: 0.8845\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1631 - accuracy: 0.9389 - val_loss: 0.3777 - val_accuracy: 0.8847\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1600 - accuracy: 0.9400 - val_loss: 0.3488 - val_accuracy: 0.8893\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1570 - accuracy: 0.9409 - val_loss: 0.3947 - val_accuracy: 0.8843\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1536 - accuracy: 0.9426 - val_loss: 0.3703 - val_accuracy: 0.8885\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1523 - accuracy: 0.9427 - val_loss: 0.3795 - val_accuracy: 0.8856\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1473 - accuracy: 0.9451 - val_loss: 0.3994 - val_accuracy: 0.8877\n",
            "[0.181, 0.1765, 0.1723, 0.1674, 0.1631, 0.16, 0.157, 0.1536, 0.1523, 0.1473]\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1444 - accuracy: 0.9460 - val_loss: 0.3884 - val_accuracy: 0.8902\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1403 - accuracy: 0.9471 - val_loss: 0.3904 - val_accuracy: 0.8896\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1379 - accuracy: 0.9488 - val_loss: 0.4018 - val_accuracy: 0.8872\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1346 - accuracy: 0.9494 - val_loss: 0.4095 - val_accuracy: 0.8904\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1321 - accuracy: 0.9507 - val_loss: 0.4246 - val_accuracy: 0.8858\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1299 - accuracy: 0.9519 - val_loss: 0.4442 - val_accuracy: 0.8860\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1276 - accuracy: 0.9519 - val_loss: 0.4205 - val_accuracy: 0.8870\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1243 - accuracy: 0.9536 - val_loss: 0.4111 - val_accuracy: 0.8924\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1235 - accuracy: 0.9531 - val_loss: 0.4329 - val_accuracy: 0.8888\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1210 - accuracy: 0.9551 - val_loss: 0.4287 - val_accuracy: 0.8901\n",
            "[0.1444, 0.1403, 0.1379, 0.1346, 0.1321, 0.1299, 0.1276, 0.1243, 0.1235, 0.121]\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1174 - accuracy: 0.9561 - val_loss: 0.4701 - val_accuracy: 0.8868\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1157 - accuracy: 0.9568 - val_loss: 0.4420 - val_accuracy: 0.8901\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1154 - accuracy: 0.9568 - val_loss: 0.4645 - val_accuracy: 0.8831\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1109 - accuracy: 0.9583 - val_loss: 0.4749 - val_accuracy: 0.8846\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1107 - accuracy: 0.9585 - val_loss: 0.4594 - val_accuracy: 0.8887\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1072 - accuracy: 0.9606 - val_loss: 0.4897 - val_accuracy: 0.8832\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1084 - accuracy: 0.9594 - val_loss: 0.4864 - val_accuracy: 0.8875\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1029 - accuracy: 0.9617 - val_loss: 0.5224 - val_accuracy: 0.8830\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1031 - accuracy: 0.9616 - val_loss: 0.5136 - val_accuracy: 0.8831\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1024 - accuracy: 0.9616 - val_loss: 0.5072 - val_accuracy: 0.8847\n",
            "[0.1174, 0.1157, 0.1154, 0.1109, 0.1107, 0.1072, 0.1084, 0.1029, 0.1031, 0.1024]\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1015 - accuracy: 0.9617 - val_loss: 0.4829 - val_accuracy: 0.8906\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0975 - accuracy: 0.9633 - val_loss: 0.5080 - val_accuracy: 0.8889\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0973 - accuracy: 0.9631 - val_loss: 0.5427 - val_accuracy: 0.8855\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0973 - accuracy: 0.9637 - val_loss: 0.5109 - val_accuracy: 0.8878\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0948 - accuracy: 0.9640 - val_loss: 0.5234 - val_accuracy: 0.8895\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0916 - accuracy: 0.9657 - val_loss: 0.5461 - val_accuracy: 0.8847\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0928 - accuracy: 0.9647 - val_loss: 0.5246 - val_accuracy: 0.8881\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0906 - accuracy: 0.9669 - val_loss: 0.5117 - val_accuracy: 0.8860\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0884 - accuracy: 0.9670 - val_loss: 0.5165 - val_accuracy: 0.8911\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0869 - accuracy: 0.9673 - val_loss: 0.5370 - val_accuracy: 0.8882\n",
            "[0.1015, 0.0975, 0.0973, 0.0973, 0.0948, 0.0916, 0.0928, 0.0906, 0.0884, 0.0869]\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0853 - accuracy: 0.9682 - val_loss: 0.5873 - val_accuracy: 0.8870\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0859 - accuracy: 0.9681 - val_loss: 0.5612 - val_accuracy: 0.8895\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0838 - accuracy: 0.9686 - val_loss: 0.5682 - val_accuracy: 0.8836\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0818 - accuracy: 0.9689 - val_loss: 0.5647 - val_accuracy: 0.8862\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0822 - accuracy: 0.9685 - val_loss: 0.5921 - val_accuracy: 0.8843\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0794 - accuracy: 0.9708 - val_loss: 0.5551 - val_accuracy: 0.8897\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0814 - accuracy: 0.9698 - val_loss: 0.5943 - val_accuracy: 0.8866\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0798 - accuracy: 0.9704 - val_loss: 0.5657 - val_accuracy: 0.8832\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0763 - accuracy: 0.9721 - val_loss: 0.5998 - val_accuracy: 0.8865\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0741 - accuracy: 0.9722 - val_loss: 0.6087 - val_accuracy: 0.8845\n",
            "[0.0853, 0.0859, 0.0838, 0.0818, 0.0822, 0.0794, 0.0814, 0.0798, 0.0763, 0.0741]\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0736 - accuracy: 0.9725 - val_loss: 0.6221 - val_accuracy: 0.8846\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0757 - accuracy: 0.9721 - val_loss: 0.5958 - val_accuracy: 0.8880\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0737 - accuracy: 0.9726 - val_loss: 0.6205 - val_accuracy: 0.8888\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0755 - accuracy: 0.9717 - val_loss: 0.6038 - val_accuracy: 0.8897\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0737 - accuracy: 0.9726 - val_loss: 0.6348 - val_accuracy: 0.8847\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0714 - accuracy: 0.9742 - val_loss: 0.6410 - val_accuracy: 0.8853\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0709 - accuracy: 0.9735 - val_loss: 0.6154 - val_accuracy: 0.8831\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0679 - accuracy: 0.9746 - val_loss: 0.6433 - val_accuracy: 0.8868\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0668 - accuracy: 0.9756 - val_loss: 0.6592 - val_accuracy: 0.8851\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0704 - accuracy: 0.9734 - val_loss: 0.6367 - val_accuracy: 0.8865\n",
            "[0.0736, 0.0757, 0.0737, 0.0755, 0.0737, 0.0714, 0.0709, 0.0679, 0.0668, 0.0704]\n"
          ]
        }
      ],
      "source": [
        "for i in range(no_of_individuals):\n",
        "        individuals.append(init())\n",
        "\n",
        "for generation in range(no_of_generations):\n",
        "    individuals, losses = train(individuals)\n",
        "    print(losses)\n",
        "\n",
        "    individuals = evolve(individuals, losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ijQz1LGXNzr",
        "outputId": "1594de99-6c4d-4880-93ba-2cacf9fb13ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.6367 - accuracy: 0.8865\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6367084383964539, 0.8865000009536743]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "model = individuals[0]\n",
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model('./my_model.h5')"
      ],
      "metadata": {
        "id": "1x7TDIXOo9pG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_test = model.predict(X_test).argmax(axis=1)\n",
        "i = numpy.random.choice(X_test.shape[0], replace=False)\n",
        "plt.imshow(X_test[i], cmap= 'gray')\n",
        "plt.title(\"True label: %s Predicted: %s\" % (y_test[i], p_test[i]))"
      ],
      "metadata": {
        "id": "EBf09w09d7gh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "01c6afae-fd1b-4802-e2bb-f0a703c265bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'True label: 2 Predicted: 2')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "8a0916fdec204e0a24c380f434be5b41b91f5eb4bf903fceed4db23957524ed9"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('genetic-algo')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}